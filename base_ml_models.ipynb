{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Remove special characters except apostrophes and hyphens\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\w\\s\\'-]\", \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dataset: 2461\n",
      "Number of unique authors: 1234\n",
      "\n",
      "Count of each sentiment label:\n",
      "y_label_sentiment\n",
      "positive    2213\n",
      "negative     248\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Count of each political lean label:\n",
      "y_label_politics\n",
      "liberal         1690\n",
      "conservative     771\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Clean data and confirm breakdown of dataset\n",
    "file_path = \"./data/cnn_articles_large_trimmed_labeled_3.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df[\"cleaned_text\"] = df[\"Article text\"].apply(preprocess_text)\n",
    "print(f\"Length of the dataset: {len(df)}\")\n",
    "\n",
    "num_unique_authors = df['Author'].nunique()\n",
    "print(f\"Number of unique authors: {num_unique_authors}\")\n",
    "\n",
    "sentiment_label_counts = df['y_label_sentiment'].value_counts()\n",
    "political_lean_label_counts = df['y_label_politics'].value_counts()\n",
    "\n",
    "print(\"\\nCount of each sentiment label:\")\n",
    "print(sentiment_label_counts)\n",
    "\n",
    "print(\"\\nCount of each political lean label:\")\n",
    "print(political_lean_label_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: Sentiment, Logistic Regression\n",
    "(Initial test with a single model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis\n",
      "Accuracy: 0.9026369168356998\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.00      0.00        48\n",
      "    positive       0.90      1.00      0.95       445\n",
      "\n",
      "    accuracy                           0.90       493\n",
      "   macro avg       0.95      0.50      0.47       493\n",
      "weighted avg       0.91      0.90      0.86       493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets for sentiment analysis\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"cleaned_text\"], df[\"y_label_sentiment\"], test_size=0.2, random_state=42)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Build a pipeline with a TF-IDF vectorizer and a logistic regression model\n",
    "pipeline = Pipeline([\n",
    "    (\"tfidf\", tfidf_vectorizer),\n",
    "    (\"clf\", LogisticRegression())\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Sentiment Analysis\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, zero_division=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: Political, Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Political Lean Analysis\n",
      "Accuracy: 0.8296146044624746\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "conservative       0.86      0.56      0.68       159\n",
      "     liberal       0.82      0.96      0.88       334\n",
      "\n",
      "    accuracy                           0.83       493\n",
      "   macro avg       0.84      0.76      0.78       493\n",
      "weighted avg       0.83      0.83      0.82       493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_politics, X_test_politics, y_train_politics, y_test_politics = train_test_split(df[\"cleaned_text\"], df[\"y_label_politics\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Build a pipeline for political lean classification\n",
    "pipeline_politics = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(max_features=5000)),\n",
    "    (\"clf\", LogisticRegression())\n",
    "])\n",
    "\n",
    "pipeline_politics.fit(X_train_politics, y_train_politics)\n",
    "\n",
    "y_pred_politics = pipeline_politics.predict(X_test_politics)\n",
    "\n",
    "print(\"Political Lean Analysis\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test_politics, y_pred_politics))\n",
    "print(classification_report(y_test_politics, y_pred_politics, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base ML Models Sentiment and Political Classification & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Support Vector Machine\": SVC(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Neural Network\": MLPClassifier(max_iter=500)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalized evaluation function for sentiment analysis and political bias\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, pos_label):\n",
    "    pipeline = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(max_features=5000)),\n",
    "        (\"clf\", model)\n",
    "    ])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, pos_label=pos_label, average=\"binary\", zero_division=1)\n",
    "    recall = recall_score(y_test, y_pred, pos_label=pos_label, average=\"binary\", zero_division=1)\n",
    "    f1 = f1_score(y_test, y_pred, pos_label=pos_label, average=\"binary\", zero_division=1)\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate each model for sentiment analysis\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"cleaned_text\"], df[\"y_label_sentiment\"], test_size=0.2, random_state=42)\n",
    "\n",
    "sentiment_metrics = {\"Model\": [], \"Accuracy\": [], \"Precision\": [], \"Recall\": [], \"F1-Score\": []}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    accuracy, precision, recall, f1 = evaluate_model(model, X_train, X_test, y_train, y_test, pos_label=\"positive\")\n",
    "    sentiment_metrics[\"Model\"].append(model_name)\n",
    "    sentiment_metrics[\"Accuracy\"].append(accuracy)\n",
    "    sentiment_metrics[\"Precision\"].append(precision)\n",
    "    sentiment_metrics[\"Recall\"].append(recall)\n",
    "    sentiment_metrics[\"F1-Score\"].append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate each model for political lean classification\n",
    "X_train_politics, X_test_politics, y_train_politics, y_test_politics = train_test_split(df[\"cleaned_text\"], df[\"y_label_politics\"], test_size=0.2, random_state=42)\n",
    "\n",
    "political_lean_metrics = {\"Model\": [], \"Accuracy\": [], \"Precision\": [], \"Recall\": [], \"F1-Score\": []}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    accuracy, precision, recall, f1 = evaluate_model(model, X_train_politics, X_test_politics, y_train_politics, y_test_politics, pos_label=\"liberal\")\n",
    "    political_lean_metrics[\"Model\"].append(model_name)\n",
    "    political_lean_metrics[\"Accuracy\"].append(accuracy)\n",
    "    political_lean_metrics[\"Precision\"].append(precision)\n",
    "    political_lean_metrics[\"Recall\"].append(recall)\n",
    "    political_lean_metrics[\"F1-Score\"].append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis Model Comparison\n",
      "                    Model  Accuracy  Precision    Recall  F1-Score\n",
      "0     Logistic Regression  0.902637   0.902637  1.000000  0.948827\n",
      "1  Support Vector Machine  0.902637   0.902637  1.000000  0.948827\n",
      "2           Random Forest  0.902637   0.902637  1.000000  0.948827\n",
      "3          Neural Network  0.910751   0.916840  0.991011  0.952484\n",
      "\n",
      "Political Lean Analysis Model Comparison\n",
      "                    Model  Accuracy  Precision    Recall  F1-Score\n",
      "0     Logistic Regression  0.829615   0.820513  0.958084  0.883978\n",
      "1  Support Vector Machine  0.809331   0.819149  0.922156  0.867606\n",
      "2           Random Forest  0.870183   0.855263  0.973054  0.910364\n",
      "3          Neural Network  0.805274   0.838068  0.883234  0.860058\n"
     ]
    }
   ],
   "source": [
    "# output_path_sentiment = \"./data/metrics_sentiment_base_models.csv\"\n",
    "# output_path_political = \"./data/metrics_political_base_models_liberal.csv\"\n",
    "\n",
    "sentiment_metrics_df = pd.DataFrame(sentiment_metrics)\n",
    "political_lean_metrics_df = pd.DataFrame(political_lean_metrics)\n",
    "\n",
    "# Display the comparison tables\n",
    "print(\"Sentiment Analysis Model Comparison\")\n",
    "print(sentiment_metrics_df)\n",
    "# sentiment_metrics_df.to_csv(output_path_sentiment, index=False)\n",
    "\n",
    "print(\"\\nPolitical Lean Analysis Model Comparison\")\n",
    "print(political_lean_metrics_df)\n",
    "# political_lean_metrics_df.to_csv(output_path_political, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
